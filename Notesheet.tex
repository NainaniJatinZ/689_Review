\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{url}
\usepackage[demo]{graphicx}
\usepackage{caption} 
\usepackage{algpseudocode}
\usepackage{bbm}
\usepackage{float}
\usepackage{framed}
\usepackage{enumerate}
\usepackage{listings} % for the code in C2
\usepackage{cancel}
\usepackage{color}
\usepackage{import}
\usepackage[colorlinks=true, linkcolor=red, urlcolor=blue, citecolor=blue]{hyperref}
\usepackage{xcolor}
\author{Jatin, Henry, AJ}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
\DeclareMathOperator*{\E}{\mathbb{E}}
\let\Pr\relax
\DeclareMathOperator*{\Pr}{\mathbb{P}}
\DeclareMathOperator*{\R}{\mathbb{E}}

\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
\title{CS 689 NOTESHEET}
\begin{document}
    \maketitle
    \section{Define supervised and unsupervised ML tasks}
    \subsection{Notation}
    
    \subsection{Classification Task Definition}
    \subsection{Regression Task Definition}
    \subsection{Regressson and Classification Prediction Function models}
    \subsection{Regression and Classification Loss}


    \section{Optimization frameworks including exact and iterative methods}
        


    \section{Algorithm, Derivations, Scalability}

    \subsection{OLS Linear regression}
    \subsection{Logistic regression}
    \subsection{Support vector classifier}
    \subsection{Support vector regression}
    \subsection{Perceptrons}
    \subsection{Basis expansions}
    \subsection{Feature selection}
    
    \section{Custom Models}
    
    \subsection{Closed form solution for custom prediction function models and/or loss functions}
    \subsection{Deriving gradients for custom prediction function models and/or loss functions}

    \section{Concepts for Generalization, Evalutation, Capacity Control}

    \subsection{Bias of an estimator}
    \subsection{Generalization}
    \subsection{Capacity control}
    \subsection{Overfitting}
    \subsection{Training error}
    \subsection{Validation error}
    \subsection{Test error}

    \section{design valid machine learning experiments}
    \subsection{Train-Test experiment design}
    \subsection{Train-Validation-Test experiment design}
    
    \section{abcdefg}
    \subsection{Interpretation of 1D regression plots}
    \subsection{Interpretation of 2D classification plots}
    \subsection{Interpretation of optimizer convergence plots}
    \subsection{Interpretation of predictive performance vs regularization strength plots}
    \section{8}
    \subsection{NumPy code vectorization}
    
    \section{9}
    \subsection{Use of approx fprime gradient checks}
    \subsection{Use of the SciPy minimize function}

\end{document}